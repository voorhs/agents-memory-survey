# Подходы

Ниже предоставлен вольный пересказ основных подходов, используемых при реализации памяти для агентов. Каждый подраздел самую малость натягивает сову на глобус ("обобщения опасны - даже это", А. Дюма), но на мой взгляд рассмотрение разных сложных подходов через призму уже знакомых понятий пускай и немного, но приближает к пониманию истины. Для более полного постижения этих методов стоит читать источники.

Память для агентов, как правило, хранится как текстовая информация (поскольку мы работаем с LLM), организованная как некоторая структура данных, позволяющая эффективный поиск и добавление данных. 

## Скользящая суммаризация

Идея скользящей суммаризации как механизма памяти проста: LLM-приложение поддерживает текстовое описание всех имеющихся данных и обновляет его с приходом новой информации. Например, скользящая суммаризация используется в [MemoryBank] и [LD-Agent] для поддержания портрета пользователя. В [MemGPT] скользящее саммари служит рабочим контекстом для решения всех задач подобно тому, как ОЗУ хранит всю информацию, необходимую для работы процессов. В [Mem0] каждый новый turn обрабатывается LLM для извлечения "воспоминаний". На основе этих воспоминаний поддерживается скользящая суммаризация текущего диалога.

Можно сказать, что такой подход полезен для отражения изменяющегося состояния отдельных объектов, но не применим для случаев, когда нужно анализировать сложные взаимосвязи между фактами, которые были актуальны в прошлом, потому что он хранит текущее состояние, но не предыдущие.

## Векторный индекс на сырых данных

Векторный индекс на сырых данных прост с точки зрения реализации. Он обеспечивает хранение данных без потерь, дает возможность отследить источник мыслей агента и опирается на эффективные алгоритмы векторного поиска. Например, [MemoryBank] и [SeCom] хранят сообщения пользователя, а не поддерживают скользящий индекс.

Главным недостатком является то, что использование сырых данных в качестве контекста для LLM не дает LLM глобального представления о всей имеющейся информации. Фрагменты документов или предыдущих диалогов — это не то, что нужно для полноценного понимания текущего состояния среды. Индексирование сырых данных опирается на то, что вся необходимая информация уже обобщена и сохранена в текстовых чанках — это условие как правило выполняется для рядового RAG по статичным документам, но никак не для динамически развивающейся среды, в которой действует агент.

Еще одним минусом является невозможность корректно обновить векторный индекс, например, в случае изменения какой-либо информации о пользователе. Если в индексе есть два противоречащих ответа на один вопрос, то LLM сильно запутается. Можно придумать костыль в виде добавления таймстемпов, но надежд, что LLM будет их учитывать, не так много.

Последние работы на тему агентской памяти не используют RAG в таком чистом виде, однако для реальных приложений такое решение может стать неплохой стартовой точкой, а для исследований — разумным бейзлайном.

## Векторный индекс на суммаризациях

Чтобы внести элемент обобщения и осмысления сырых данных, некоторые работы предлагают хранить и индексировать их суммаризации. Например, [Theanine], [LD-Agent] и [Nemori] индексируют суммаризированные "воспоминания"/"эпизоды" из диалогов. [A-Mem] индексирует конкатенацию сырых данных с их суммаризациями. [RMM] хранит и куски диалога, и их суммаризации, при этом векторный поиск осуществляется только по последней.

Такой подход очень логичен, поскольку позволяет уменьшить шум в хранилище и получить компактные текстовые репрезентации знаний о среде. Однако такая структура данных требует постоянных вызовов LLM, что может повысить задержку в real-time приложениях. [A-Mem] пытается обойти эту проблему тем, что генерирует суммаризации асинхронно в фоне. Но мне кажется, это может вызывать data race из-за работы с запаздывающей памятью на запись/чтение. 

## Иерархическая кластеризация

Иерархическая кластеризация имеет прямой целью обобщить сырые данные, чтобы получить глобальное представление об имеющихся данных. Например, в [RAPTOR] эмбединги сырых данных кластеризуются рекурсивно, до получения древовидной структуры. Причем каждый кластер презентуется не центроидом, а LLM-generated суммаризацией входящих в него элементов. Чем ближе суммаризация к корню дерева, тем глобальнее получается знание. Суммаризации со всех уровней участвуют в векторном поиске, что позволяет учесть все уровни гранулярности: от сырых представлений до высокоуровневых обобщений.

Подход с иерархической кластеризацией можно рассмотреть как комбинацию рассмотренных ранее подходов, векторного индекса на сырых данных и векторного индекса на суммаризациях. Однако опять же, постоянная суммаризация оборачивается либо повышенной задержкой, либо data race.

Важнейшим недостатком такого подхода является дороговизна обновления этой иерархической структуры данных: если пришли новые факты, которые требуют перегруппировки кластеров, то нужно заново запускать рекурсивную кластеризацию и суммаризацию.

Также мне кажется, что такой подход очень уж чувствителен к галлюцинациям LLM, поскольку тут делается очень много LLM-вызовов для суммаризации, которая является сложной задачей.

## Граф сущностей

Граф сущностей — это способ организации информации в виде набора триплетов `(сущность_1, отношение, сущность_2)`, где сущности формируют вершины, а отношения — ребра. Такая структура данных означает конвертацию сырых данных в некоторое более структурированное представление. Извлечение сущностей и отношений между ними — это способ добавить компонент осмысления. Это полный отход от поиска по сырым данным, поскольку индексируются только узлы и ребра. Такой подход используется в [GraphRAG], [Zep], опционально в [Mem0].

В граф сущностей можно добавить суммаризацию и иерархическую кластеризацию, как это сделано в [Zep]. Это добавляет соответствующие плюсы и минусы. Авторы Zep постарались смягчить накладные расходы на перестроение иерархии и перегенерацию суммаризации с помощью некоторой эвристики, но в корне эту проблему не решили.

Графовая структура позволяет расширить набор кандидатов-воспоминаний, которые будут участвовать в формировании контекста LLM во время генерации ответа, — можно использовать подграфы, образованные вокруг релевантных данных. Например, в [Zep] строится подграф на основе последних $m$ сообщений в диалоге.

## Граф событий

Вместо тотальной конвертации сырых данных в абстрактную сеть сущностей, можно моделировать связи между сырыми данными напрямую. В статье [A-Mem] предложено организовать хранилище наподобие того, как организуют заметки по методу Zettelkasten. В рамках этого метода каждый новый кусок информации добавляется как новая вершина графа, причем контент вершины это не сущность, а сырые данные в исходной и/или суммаризированной формах. Новая вершина соединяется с релевантными вершинами, которые уже есть в графе.

Такой граф, как в A-Mem, позволяет производить обновление имеющихся знаний немного более точечно: вместо перестроения всей иерархии, достаточно обновить контент вершин, соседних с новоприбывшей вершиной. При этом индексируются сами воспоминания, и в контекст LLM идут только непосредственно извлеченные воспоминания, но не их соседи. То есть графовая структура нужно исключительно для того, чтобы поддерживать актуальную информацию, но не для поиска контекста.

Такой алгоритм, на мой взгляд, может стать минусом, т.к. глубина соседства, которую нужно рассматривать при обновлении — это гиперпараметр, который нужно подбирать. К тому же, поскольку тут нет иерархии, то проблема выбора гранулярности поиска не решена.

Похожий подход реализован в [Theanine]. В этой статье воспоминания тоже хранятся в виде вершин, связанных друг с другом по смыслу, однако ребра между вершинами являются направленными и отражают отношение предшествования. Это позволяет отказаться от обновления графа, так как теперь при генерации ответа можно достать целый таймлайн и на месте понять, что сейчас актуально, а что нет. Можно воспринимать такой подход как RAG, в котором чанки просто становятся связаны по смыслу и времени, потому что на мой взгляд Theanine обладает минусами векторного индекса сырых/суммаризированных данных и может давать много галлюцинаций.

## Параметрическая память

В статье [RMM] представлен RL-подход для персонализации. Он заключается в дообучении реранкера воспоминаний на основе LLM-as-a-judge оценок релевантности выдачи для пользователя. Это единственный пример параметрической, а не текстовой памяти агента, который я встретил.
